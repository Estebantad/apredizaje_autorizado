{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060b11a-6a02-4054-9075-6346654514ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  4. Machine Learning (4_machine_learning.ipynb)\n",
    "\n",
    "# Importar paquetes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model Selection y M茅tricas\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Pipelines (Aunque aqu铆 no se usa el objeto Pipeline, s铆 se usa la l贸gica de entrenamiento)\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "# Para guardar el modelo\n",
    "import pickle\n",
    "\n",
    "# --- 1. Carga de datos procesados ---\n",
    "df = pd.read_csv('./data/titanic_procesado.csv')\n",
    "\n",
    "# --- 2. Divisi贸n de datos (X y Y) ---\n",
    "X = df.drop(['Survived'], axis=1) # Caracter铆sticas\n",
    "y = df['Survived']               # Variable Objetivo\n",
    "\n",
    "# --- 3. Divisi贸n Entrenamiento-Prueba (80/20) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir a NumPy arrays (como se sugiri贸 en el proceso)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "\n",
    "print(f\"Dimensiones de entrenamiento: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Dimensiones de prueba: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "# --- 4. Definici贸n de Modelos e Hiperpar谩metros para GridSearch ---\n",
    "modelos = {\n",
    "    'Regresi贸n Log铆stica': {\n",
    "        'modelo': LogisticRegression(solver='liblinear'),\n",
    "        'parametros': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Vectores de Soporte': {\n",
    "        'modelo': SVC(random_state=42),\n",
    "        'parametros': {\n",
    "            'kernel': ['linear', 'rbf'], # Reducido para velocidad\n",
    "            'C': [0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de rbol de Decisi贸n': {\n",
    "        'modelo': DecisionTreeClassifier(random_state=42),\n",
    "        'parametros': {\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [None, 3, 5] # Reducido\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Bosques Aleatorios': {\n",
    "        'modelo': RandomForestClassifier(random_state=42),\n",
    "        'parametros': {\n",
    "            'n_estimators': [10, 100],\n",
    "            'max_depth': [None, 3, 5],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Gradient Boosting': {\n",
    "        'modelo': GradientBoostingClassifier(random_state=42),\n",
    "        'parametros': {\n",
    "            'n_estimators': [10, 100],\n",
    "            'max_depth': [1, 3] # Reducido\n",
    "        }\n",
    "    },\n",
    "    'Clasificador AdaBoost': {\n",
    "        'modelo': AdaBoostClassifier(random_state=42),\n",
    "        'parametros': {\n",
    "            'n_estimators': [10, 100]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador K-Nearest Neighbors': {\n",
    "        'modelo': KNeighborsClassifier(),\n",
    "        'parametros': {\n",
    "            'n_neighbors': [3, 5, 7] # Hiperpar谩metro clave\n",
    "        }\n",
    "    },\n",
    "    'Clasificador XGBoost': {\n",
    "        'modelo': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "        'parametros': {\n",
    "            'n_estimators': [10, 100],\n",
    "            'max_depth': [1, 3]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador LGBM': {\n",
    "        'modelo': LGBMClassifier(random_state=42, verbose=-1),\n",
    "        'parametros': {\n",
    "            'n_estimators': [10, 100],\n",
    "            'max_depth': [1, 3],\n",
    "        }\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'modelo': GaussianNB(),\n",
    "        'parametros': {}\n",
    "    },\n",
    "    'Clasificador Naive Bayes': {\n",
    "        'modelo': BernoulliNB(),\n",
    "        'parametros': {\n",
    "            'alpha': [0.1, 1.0] # Reducido\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- 5. Entrenamiento y Selecci贸n del Mejor Modelo (GridSearch) ---\n",
    "puntajes_modelos = []\n",
    "mejor_precision = 0\n",
    "mejor_estimador = None\n",
    "mejor_modelo = None\n",
    "\n",
    "for nombre, info_modelo in modelos.items():\n",
    "    print(f\"Entrenando {nombre}...\")\n",
    "    \n",
    "    # 5.1. Inicializar GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=info_modelo['modelo'],\n",
    "        param_grid=info_modelo['parametros'],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        verbose=0,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # 5.2. Ajustar GridSearchCV con los datos de entrenamiento\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # 5.3. Hacer predicciones\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    \n",
    "    # 5.4. Calcular la precisi贸n\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # 5.5. Almacenar resultados\n",
    "    puntajes_modelos.append({\n",
    "        'Modelo': nombre,\n",
    "        'Precisi贸n': precision\n",
    "    })\n",
    "    \n",
    "    # 5.6. Actualizar el mejor modelo\n",
    "    if precision >= mejor_precision: # Usamos >= para incluir empates si se desea\n",
    "        mejor_modelo = nombre\n",
    "        mejor_precision = precision\n",
    "        mejor_estimador = grid_search.best_estimator_\n",
    "        # Imprimir resultados del mejor modelo actual para seguimiento\n",
    "        print(f\"Nuevo Mejor Modelo: {mejor_modelo} con Precisi贸n: {mejor_precision:.4f}\")\n",
    "\n",
    "# --- 6. Mostrar Resultados Finales ---\n",
    "metricas = pd.DataFrame(puntajes_modelos).sort_values('Precisi贸n', ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Rendimiento de los modelos de clasificaci贸n\")\n",
    "print(metricas.round(4))\n",
    "print(\"=\"*50)\n",
    "print(\"MEJOR MODELO DE CLASIFICACIN (Resultado del GridSearch)\")\n",
    "print(f\"Modelo: {mejor_modelo}\")\n",
    "print(f\"Precisi贸n: {mejor_precision:.4f}\")\n",
    "\n",
    "# --- 7. Guardar el Mejor Modelo ---\n",
    "# NOTA: En este notebook guardamos el modelo de la versi贸n \"manual\" (sin Pipeline)\n",
    "with open('modelo_ml_entrenado.pkl', 'wb') as archivo_estimador:\n",
    "    pickle.dump(mejor_estimador, archivo_estimador)\n",
    "print(f\"\\nEl mejor estimador ({mejor_modelo}) guardado como: modelo_ml_entrenado.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
